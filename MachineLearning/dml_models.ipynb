{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-16 17:12:58.925172: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Library requirements \n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.initializers import HeNormal, GlorotUniform, GlorotNormal\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DML Models \n",
    "Selection of Differential Machine learning model architctures used for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autodiff(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer for automatic differentiation of a forward model's output with respect to its inputs.\n",
    "\n",
    "    Computes the gradients of the forward model's predictions with respect to the input features,\n",
    "    allowing for gradient-based optimization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fwd_model : keras.Model: forward model whose output gradients with respect to the inputs are to be computed.\n",
    "    \n",
    "    kwargs : dict: additional keyword arguments passed to the Keras Layer base class.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    call(input):\n",
    "        Computes the gradient of the forward model's output with respect to the input features.\n",
    "    \"\"\"\n",
    "    def __init__(self, fwd_model, **kwargs):\n",
    "      super(Autodiff, self).__init__(**kwargs)\n",
    "      self.units = None\n",
    "      self.fwd_model = fwd_model \n",
    "    \n",
    "    def call(self, input):\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(input)\n",
    "            pred_value = self.fwd_model(input)\n",
    "           \n",
    "        # Get the gradients dy/dx_i for each x feature x_i inputs \n",
    "        gradient = tape.gradient(pred_value, input)\n",
    "\n",
    "        return gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model_dml(model_name, input_dim):\n",
    "    \"\"\"\n",
    "    Builds and returns a multi-layer perceptron neural network model based on the specified model name.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str: name of the model to build. ('model_dml', 'model_dml_BN', 'model_dml_big_BN')\n",
    "    X_train : DataFrame: training data used to determine the input shape of the model.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    A Keras Sequential model configured based on the specified model name.\n",
    "    \"\"\"\n",
    "    if model_name == 'model_dml':\n",
    "        # feedforward\n",
    "        input_1 = Input(shape=(input_dim,))\n",
    "        x = Dense(20, kernel_initializer='glorot_normal', activation='softplus')(input_1)\n",
    "        x = Dense(20, kernel_initializer='glorot_normal', activation='softplus')(x)\n",
    "        x = Dense(20, kernel_initializer='glorot_normal', activation='softplus')(x)\n",
    "        x = Dense(20, kernel_initializer='glorot_normal', activation='softplus')(x)\n",
    "        y_pred = Dense(1, kernel_initializer='glorot_normal', activation='linear', name='y_pred')(x)\n",
    "\n",
    "        fwd_model = tf.keras.models.Model(inputs=input_1, outputs=y_pred)\n",
    "\n",
    "        # automatic differentiation\n",
    "        autodiff_layer = Autodiff(fwd_model, name='dydx_pred')\n",
    "        dydx_pred = autodiff_layer(input_1)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_1, outputs=[y_pred, dydx_pred], name='Autodiff_Model')\n",
    "\n",
    "        return model\n",
    "\n",
    "    elif model_name == 'model_dml_BN':\n",
    "        # feedforward\n",
    "        input_1 = Input(shape=(input_dim,))\n",
    "        x = Dense(20, kernel_initializer='glorot_normal', activation='softplus')(input_1)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(20, kernel_initializer='glorot_normal', activation='softplus')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(20, kernel_initializer='glorot_normal', activation='softplus')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(20, kernel_initializer='glorot_normal', activation='softplus')(x)\n",
    "        y_pred = Dense(1, kernel_initializer='glorot_normal', activation='linear', name='y_pred')(x)\n",
    "\n",
    "        fwd_model = tf.keras.models.Model(inputs=input_1, outputs=y_pred)\n",
    "\n",
    "        # automatic differentiation\n",
    "        autodiff_layer = Autodiff(fwd_model, name='dydx_pred')\n",
    "        dydx_pred = autodiff_layer(input_1)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_1, outputs=[y_pred, dydx_pred], name='Autodiff_Model')\n",
    "\n",
    "        return model\n",
    "\n",
    "    elif model_name == 'model_dml_big_BN':\n",
    "        # feedforward\n",
    "        input_1 = Input(shape=(input_dim,))\n",
    "        x = Dense(30, kernel_initializer='glorot_normal', activation='softplus')(input_1)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(30, kernel_initializer='glorot_normal', activation='softplus')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(30, kernel_initializer='glorot_normal', activation='softplus')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(30, kernel_initializer='glorot_normal', activation='softplus')(x)\n",
    "        y_pred = Dense(1, kernel_initializer='glorot_normal', activation='linear', name='y_pred')(x)\n",
    "\n",
    "        fwd_model = tf.keras.models.Model(inputs=input_1, outputs=y_pred)\n",
    "\n",
    "        # automatic differentiation\n",
    "        autodiff_layer = Autodiff(fwd_model, name='dydx_pred')\n",
    "        dydx_pred = autodiff_layer(input_1)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_1, outputs=[y_pred, dydx_pred], name='Autodiff_Model')\n",
    "\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model name: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalisationLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer for normalizing inputs and outputs\n",
    "\n",
    "    Normalizes the inputs and outputs of the model using mean and sd \n",
    "    Provides methods to scale and inverse scale both the outputs and the gradients.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    muX : array: Mean of the input features computed from the training data.\n",
    "    stdX : array: Standard deviation of the input features computed from the training data.\n",
    "    muY : array: Mean of the output values computed from the training data.\n",
    "    stdY : array: Standard deviation of the output values computed from the training data.\n",
    "    n : int: Number of input features.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    adapt(x_raw, y_raw, dydx_raw):\n",
    "        Computes the mean and standard deviation for the inputs and outputs based on the training data.\n",
    "    \n",
    "    call(inputs):\n",
    "        Normalizes the inputs based on the computed mean and standard deviation.\n",
    "    \n",
    "    yScaled(y):\n",
    "        Scales the outputs based on the computed mean and standard deviation.\n",
    "    \n",
    "    yScaledInverse(y):\n",
    "        Inverse scales the outputs based on the computed mean and standard deviation.\n",
    "    \n",
    "    dydxScaled(dydx):\n",
    "        Scales the gradients based on the computed mean and standard deviation.\n",
    "    \n",
    "    dydxScaledInverse(dydx_scaled):\n",
    "        Inverse scales the gradients based on the computed mean and standard deviation.\n",
    "    \n",
    "    output_n():\n",
    "        Returns the number of input features.\n",
    "    \n",
    "    get_config():\n",
    "        Returns the configuration of the layer for serialization.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NormalisationLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.muX = 0\n",
    "        self.muY = 0 \n",
    "        self.stdX = 0\n",
    "        self.stdY = 0\n",
    "\n",
    "        self.n = 0\n",
    "\n",
    "    def adapt(self, x_raw, y_raw, dydx_raw):\n",
    "        # basic processing (step 1 in the note)\n",
    "        \n",
    "        x0 = x_raw\n",
    "        y0 = y_raw\n",
    "        x0Bar = dydx_raw\n",
    "        \n",
    "        self.n = x_raw.shape[1] \n",
    "        m = x_raw.shape[0] # size of training set\n",
    "\n",
    "        # normalize inputs\n",
    "        # compute\n",
    "        self.muX = x0.mean(axis=0)\n",
    "        self.stdX = x0.std(axis=0)\n",
    "       \n",
    "        # normalize inputs\n",
    "        # compute\n",
    "        self.muY = y0.mean(axis=0)\n",
    "        self.stdY = y0.std(axis=0)\n",
    "        \n",
    "\n",
    "    def call(self, inputs):\n",
    "        # layer called on x as inputs\n",
    "        return (inputs - self.muX) /  self.stdX\n",
    "    \n",
    "    def yScaled(self, y):\n",
    "        return (y - self.muY) / self.stdY\n",
    "\n",
    "    def yScaledInverse(self, y):\n",
    "        return (y * self.stdY ) + self.muY\n",
    "\n",
    "    def dydxScaled(self, dydx):\n",
    "        return dydx * (self.stdX / self.stdY)\n",
    "    \n",
    "    def dydxScaledInverse(self, dydx_scaled):\n",
    "        return dydx_scaled * (self.stdY / self.stdX)\n",
    "    \n",
    "    def output_n(self):\n",
    "        return self.n\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(NormalisationLayer, self).get_config()\n",
    "        return config\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grad_custom_loss(keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Custom loss function that incorporates gradient normalization\n",
    "    Loss function normalizes the gradients of the predicted values during training\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    norm_weights : array or None, optional: Weights for normalizing the gradients. If None, it will be computed from the training data.\n",
    "    reduction : str, optional: Type of reduction to apply to loss. Default is `keras.losses.Reduction.AUTO`.\n",
    "    name : str, optional: Name of the custom loss function  \"grad_custom_loss\".\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    adapt(dydx_train):\n",
    "        Adapts the normalization weights based on the gradients from the training data.\n",
    "    \n",
    "    call(y_true, y_pred):\n",
    "        Computes the custom loss as the mean squared error between the true and predicted values, scaled by the normalization weights.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, norm_weights=None, reduction=keras.losses.Reduction.AUTO, name=\"grad_custom_loss\"):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.norm_weights = norm_weights\n",
    "\n",
    "    def adapt(self, dydx_train):\n",
    "        arg = tf.convert_to_tensor(dydx_train, dtype=tf.float32)\n",
    "        self.norm_weights = 1.0 / tf.reshape(tf.sqrt(tf.reduce_mean(arg ** 2, axis=0)), [1, -1])\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.math.reduce_mean(tf.square((y_true - y_pred) * self.norm_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def error_bars_first_order_estimate(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Computes first-order error bars for a given test set based on the nearest neighbors in the INDEX training set\n",
    "    and the training sets ground truth and predicted graidents at the training points.\n",
    "\n",
    "    Calculates various error metrics and first-order estimates for the market-to-market (MtM) values\n",
    "    based on the spread and maturity duration, comparing the predicted MtM with the ground truth.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_train : DataFrame: training dataset containing the true and predicted values of cs01 and theta, along with other features.\n",
    "    df_test : DataFrame: test dataset containing the true and predicted values of MtM, along with other features.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : DataFrame: containing the combined training and test datasets along with the calculated error metrics and first-order estimates.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate errors\n",
    "    df_train['cs01_error'] = np.abs(df_train['cs01'] - df_train['cs01_pred'])\n",
    "    df_train['theta_error'] = np.abs(df_train['theta'] - df_train['theta_pred'])\n",
    "\n",
    "    train_matrix = df_train[['spread', 'mat_duration', 'coupon', 'recovery']].values\n",
    "    test_matrix = df_test[['spread', 'mat_duration', 'coupon', 'recovery']].values\n",
    "\n",
    "    # Find the nearest neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(train_matrix)\n",
    "    distances, indices = nbrs.kneighbors(test_matrix)\n",
    "\n",
    "    # For results \n",
    "    nearest_indices = indices.flatten()\n",
    "    results_df = pd.concat([\n",
    "        df_train.iloc[nearest_indices].reset_index(drop=True),\n",
    "        df_test[['spread', 'mat_duration', 'coupon', 'recovery', 'mtm', 'mtm_pred']].reset_index(drop=True).rename(columns={\n",
    "            'spread': 'spread_t',\n",
    "            'mat_duration': 'mat_duration_t',\n",
    "            'coupon': 'coupon_t',\n",
    "            'recovery': 'recovery_t',\n",
    "            'mtm': 'mtm_t',\n",
    "            'mtm_pred': 'mtm_t_pred'\n",
    "        })\n",
    "    ], axis=1)\n",
    "\n",
    "    ### New metrics to enable the error bar calculation ###\n",
    "    results_df['ds'] = results_df['spread_t'] - results_df['spread']\n",
    "    results_df['dt'] = results_df['mat_duration_t'] - results_df['mat_duration']\n",
    "\n",
    "    # fo (ground truth) estimate of price\n",
    "    results_df['mtm_gt_fo'] = results_df['mtm'] + (results_df['cs01'] * results_df['ds'] + results_df['theta'] * results_df['dt'])\n",
    "\n",
    "    # foplus estimate of price\n",
    "    results_df['mtm_gt_foplus'] = results_df['mtm'] + (\n",
    "        (results_df['cs01'] + results_df['cs01_error']) * results_df['ds'] +\n",
    "        (results_df['theta'] - results_df['theta_error']) * results_df['dt'])\n",
    "    \n",
    "\n",
    "\n",
    "    # Construct error bar\n",
    "    results_df['error_bar_size'] = np.abs(results_df['mtm_gt_foplus'] - results_df['mtm_t_pred'])  + np.abs(results_df['mtm_t'] - results_df['mtm_gt_fo'])\n",
    "    results_df['mtm_t_pred_up'] = results_df['mtm_t_pred'] + results_df['error_bar_size']\n",
    "    results_df['mtm_t_pred_dn'] = results_df['mtm_t_pred'] - results_df['error_bar_size']\n",
    "    results_df['error_bar_size_gt'] = np.abs(results_df['mtm_gt_fo'] - results_df['mtm_t'])\n",
    "    results_df['mtm_t_up'] = results_df['mtm_t'] + results_df['error_bar_size_gt']\n",
    "    results_df['mtm_t_dn'] = results_df['mtm_t'] - results_df['error_bar_size_gt']\n",
    "\n",
    "    ### Define some error metrics for info only ###\n",
    "    results_df['gt_nn_error'] = (results_df['mtm_t'] - results_df['mtm_t_pred']) / results_df['mtm_t'] * 100\n",
    "    results_df['gt_fo_error'] = (results_df['mtm_gt_fo'] - results_df['mtm_t']) / results_df['mtm_t'] * 100\n",
    "    results_df['nn_fo_error'] = (results_df['mtm_gt_fo'] - results_df['mtm_t_pred']) / results_df['mtm_t_pred'] * 100\n",
    "\n",
    "    # Check if mtm_t prediction falls within the error bars\n",
    "    results_df['mtm_t_pred_within_nn_error'] = (results_df['mtm_t_pred'] <= results_df['mtm_t_pred_up']) & (results_df['mtm_t_pred'] >= results_df['mtm_t_pred_dn'])\n",
    "    results_df['mtm_t_within_nn_error'] = (results_df['mtm_t'] <= results_df['mtm_t_pred_up']) & (results_df['mtm_t'] >= results_df['mtm_t_pred_dn'])\n",
    "    results_df['mtm_t_pred_within_gt_error'] = (results_df['mtm_t_pred'] <= results_df['mtm_t_up']) & (results_df['mtm_t_pred'] >= results_df['mtm_t_dn'])\n",
    "    results_df['mtm_t_within_gt_error'] = (results_df['mtm_t'] <= results_df['mtm_t_up']) & (results_df['mtm_t'] >= results_df['mtm_t_dn'])\n",
    "\n",
    "    # How close to the error bar are the points that are not within the error bar\n",
    "    results_df.loc[~results_df['mtm_t_within_nn_error'], 'true_misses'] = np.minimum(\n",
    "        np.abs(results_df['mtm_t'] - results_df['mtm_t_pred_up']),\n",
    "        np.abs(results_df['mtm_t'] - results_df['mtm_t_pred_dn'])\n",
    "    ) / (2 * results_df['error_bar_size']) * 100\n",
    "\n",
    "    # Size of error bars as a percentage of the prediction price\n",
    "    results_df['error_bar_percent_of_price'] = (2 * results_df['error_bar_size']) / np.abs(results_df['mtm_t_pred']) * 100\n",
    "    results_df['rel_size'] = np.abs(results_df['mtm_t_pred_up'] - results_df['mtm_t_pred_dn']) / np.abs(results_df['mtm_t_up'] - results_df['mtm_t_dn'])\n",
    "\n",
    "    # Return the results data frame\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def error_bars_first_order_estimate_opt(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Computes first-order error bars for a given test set based on the nearest neighbors in the OPTION training set\n",
    "    and the training sets ground truth and predicted graidents at the training points.\n",
    "\n",
    "    Calculates various error metrics and first-order estimates for the market-to-market (MtM) values\n",
    "    based on the spread and expiry duration, strike and implied volatility; comparing the predicted MtM with the ground truth.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_train : DataFrame: training dataset containing the true and predicted values of cs01 and theta, along with other features.\n",
    "    df_test : DataFrame: test dataset containing the true and predicted values of MtM, along with other features.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results_df : DataFrame: containing the combined training and test datasets along with the calculated error metrics and first-order estimates.\n",
    "\n",
    "    \"\"\"\n",
    "  \n",
    "    # Calculate errors\n",
    "    df_train['delta_error'] = np.abs(df_train['delta'] - df_train['delta_pred'])\n",
    "    df_train['theta_error'] = np.abs(df_train['theta'] - df_train['theta_pred'])\n",
    "    df_train['deltastrike_error'] = np.abs(df_train['deltastrike'] - df_train['deltastrike_pred'])\n",
    "    df_train['vega_error'] = np.abs(df_train['vega'] - df_train['vega_pred'])\n",
    "  \n",
    "    train_matrix = df_train[['spread', 'exp_duration', 'strike', 'impliedVol']].values\n",
    "    test_matrix = df_test[['spread', 'exp_duration', 'strike', 'impliedVol']].values\n",
    "  \n",
    "    # Find the nearest neighbors\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(train_matrix)\n",
    "    distances, indices = nbrs.kneighbors(test_matrix)\n",
    "  \n",
    "    # Extract the indices of the nearest neighbors\n",
    "    nearest_indices = indices.flatten()\n",
    "  \n",
    "    # For the results\n",
    "    results_df = pd.concat([\n",
    "        df_train.iloc[nearest_indices].reset_index(drop=True),\n",
    "        df_test[['spread', 'exp_duration', 'mat_duration', 'strike', 'impliedVol', 'mtm', 'mtm_pred']].reset_index(drop=True).rename(columns={\n",
    "            'spread': 'spread_t',\n",
    "            'exp_duration': 'exp_duration_t',\n",
    "            'mat_duration': 'mat_duration_t',\n",
    "            'strike': 'strike_t',\n",
    "            'impliedVol': 'impliedVol_t',\n",
    "            'mtm': 'mtm_t',\n",
    "            'mtm_pred': 'mtm_t_pred'\n",
    "        })\n",
    "    ], axis=1)\n",
    "  \n",
    "    ### New metrics to enable the error bar calculation ###\n",
    "    results_df['ds'] = results_df['spread_t'] - results_df['spread']\n",
    "    results_df['dt'] = results_df['exp_duration_t'] - results_df['exp_duration']\n",
    "    results_df['dk'] = results_df['strike_t'] - results_df['strike']\n",
    "    results_df['dv'] = results_df['impliedVol_t'] - results_df['impliedVol']\n",
    "  \n",
    "    # fo (ground truth) estimate of price\n",
    "    results_df['mtm_gt_fo'] = results_df['mtm'] + (results_df['delta'] * results_df['ds'] + results_df['theta'] * results_df['dt'] + results_df['deltastrike'] * results_df['dk'] + results_df['vega'] * results_df['dv'])\n",
    "  \n",
    "    # foplus estimate of price\n",
    "    results_df['mtm_gt_foplus'] = results_df['mtm'] + (\n",
    "        (results_df['delta'] + results_df['delta_error']) * results_df['ds'] +\n",
    "        (results_df['theta'] + results_df['theta_error']) * results_df['dt'] +\n",
    "        (results_df['deltastrike'] + results_df['deltastrike_error']) * results_df['dk'] +\n",
    "        (results_df['vega'] + results_df['vega_error']) * results_df['dv']\n",
    "    )\n",
    "  \n",
    "    # Construct error bar\n",
    "    results_df['error_bar_size'] = np.abs(results_df['mtm_gt_foplus'] - results_df['mtm_t_pred'])  + np.abs(results_df['mtm_t'] - results_df['mtm_gt_fo'])\n",
    "    results_df['mtm_t_pred_up'] = results_df['mtm_t_pred'] + results_df['error_bar_size']\n",
    "    results_df['mtm_t_pred_dn'] = results_df['mtm_t_pred'] - results_df['error_bar_size']\n",
    "    results_df['error_bar_size_gt'] = np.abs(results_df['mtm_gt_fo'] - results_df['mtm_t'])\n",
    "    results_df['mtm_t_up'] = results_df['mtm_t'] + results_df['error_bar_size_gt']\n",
    "    results_df['mtm_t_dn'] = results_df['mtm_t'] - results_df['error_bar_size_gt']\n",
    "  \n",
    "    ### Define the error metrics ###\n",
    "    # The 'real error' in prediction between the real gt price and the nn prediction\n",
    "    results_df['gt_nn_error'] = (results_df['mtm_t'] - results_df['mtm_t_pred']) / results_df['mtm_t'] * 100\n",
    "    # Error between gt price and the gt fo estimate of the price\n",
    "    results_df['gt_fo_error'] = (results_df['mtm_gt_fo'] - results_df['mtm_t']) / results_df['mtm_t'] * 100\n",
    "    # Error between nn prediction and gt fo estimate (gt fo estimate could be used as a floor/cap of mtm_t_pred)\n",
    "    results_df['nn_fo_error'] = (results_df['mtm_gt_fo'] - results_df['mtm_t_pred']) / results_df['mtm_t_pred'] * 100\n",
    "  \n",
    "    # Remove rows where ds = dt = dk = dv = 0\n",
    "    #results_df = results_df[~((results_df['ds'] == 0) & (results_df['dt'] == 0) & (results_df['dk'] == 0) & (results_df['dv'] == 0))]\n",
    "  \n",
    "    # Check if mtm_t prediction falls within the error bars\n",
    "    results_df['mtm_t_pred_within_nn_error'] = (results_df['mtm_t_pred'] <= results_df['mtm_t_pred_up']) & (results_df['mtm_t_pred'] >= results_df['mtm_t_pred_dn'])\n",
    "    results_df['mtm_t_within_nn_error'] = (results_df['mtm_t'] <= results_df['mtm_t_pred_up']) & (results_df['mtm_t'] >= results_df['mtm_t_pred_dn'])\n",
    "    results_df['mtm_t_pred_within_gt_error'] = (results_df['mtm_t_pred'] <= results_df['mtm_t_up']) & (results_df['mtm_t_pred'] >= results_df['mtm_t_dn'])\n",
    "    results_df['mtm_t_within_gt_error'] = (results_df['mtm_t'] <= results_df['mtm_t_up']) & (results_df['mtm_t'] >= results_df['mtm_t_dn'])\n",
    "  \n",
    "    # How close to the error bar are the points that are not within the error bar\n",
    "    results_df.loc[~results_df['mtm_t_within_nn_error'], 'true_misses'] = np.minimum(\n",
    "        np.abs(results_df['mtm_t'] - results_df['mtm_t_pred_up']),\n",
    "        np.abs(results_df['mtm_t'] - results_df['mtm_t_pred_dn'])\n",
    "    ) / (2 * results_df['error_bar_size']) * 100\n",
    "  \n",
    "    # Size of error bars as a percentage of the prediction price\n",
    "    results_df['error_bar_percent_of_price'] = (2 * results_df['error_bar_size']) / np.abs(results_df['mtm_t_pred']) * 100\n",
    "    results_df['rel_size'] = np.abs(results_df['mtm_t_pred_up'] - results_df['mtm_t_pred_dn']) / np.abs(results_df['mtm_t_up'] - results_df['mtm_t_dn'])\n",
    "  \n",
    "    # Return the results data frame\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x_train, y_train, dydx_train, prep_type='Normalisation'):\n",
    "    \"\"\"\n",
    "    Preprocesses the training data. Only one preparation type currently. \n",
    "\n",
    "    Normalizes the input features and gradients, and initializes a custom gradient-based loss function\n",
    "    Other preparation types can be added if required\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x_train : array or DataFrame: training input features.\n",
    "    y_train : array or DataFrame: training target values.\n",
    "    dydx_train : array or DataFrame: gradients of the training data.\n",
    "    prep_type : str, optional: type of data preparation to perform. ()'Normalisation')\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    prep_layer : NormalisationLayer: normalization layer initialized and adapted to the training data.\n",
    "    GradLoss : grad_custom_loss: custom gradient-based loss function adapted to the normalized gradients.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if prep_type == 'Normalisation':\n",
    "        prep_layer = NormalisationLayer(input_shape=[x_train.shape[1],])    \n",
    "        prep_layer.adapt(x_train, y_train, dydx_train)\n",
    "\n",
    "        GradLoss = grad_custom_loss()\n",
    "        GradLoss.adapt(prep_layer(dydx_train))\n",
    "    \n",
    "        return prep_layer, GradLoss\n",
    "    \n",
    "    # Add other scaling if need be here ...\n",
    "    else:\n",
    "         raise ValueError(f\"Unsupported data preparation type: {prep_type}\")\n",
    "    \n",
    "\n",
    "    \n",
    "# predict and inverse data transform   \n",
    "def predict_unscaled(model, prep_layer, x_unscaled):\n",
    "    \"\"\"\n",
    "    Makes predictions using a trained model on unscaled input data and returns the unscaled predictions and gradients.\n",
    "\n",
    "    First scales the input data using the provided preprocessing layer, makes predictions using the\n",
    "    provided model, and then inversely scales the predictions and gradients to return them in their original scale.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model: trained Keras model used for making predictions.\n",
    "    prep_layer : NormalisationLayer: normalization layer used to scale and inverse scale the input data and predictions.\n",
    "    x_unscaled : array or DataFrame: unscaled input data for which predictions are to be made.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    y_pred : array: unscaled predicted values.\n",
    "    dydx_pred : array: unscaled predicted gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    y_scaled, dydx_scaled = model.predict(prep_layer(x_unscaled))\n",
    "    y_pred = prep_layer.yScaledInverse(y_scaled)\n",
    "    dydx_pred = prep_layer.dydxScaledInverse(dydx_scaled)\n",
    "    \n",
    "    return y_pred.reshape(-1,1), dydx_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Build, Compile and Fit Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_model(\n",
    "        input_dim,\n",
    "        model_name,\n",
    "        grad_loss,\n",
    "        differential_weight,\n",
    "        lr_schedule\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Builds and compiles a model with the specified parameters, including a custom gradient-based loss function and learning rate schedule.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int: number of input features for the model.\n",
    "    model_name : str: name of the model. \n",
    "    grad_loss : grad_custom_loss: custom gradient-based loss function to use for training.\n",
    "    differential_weight : float:  weight factor for balancing the main loss and gradient loss.\n",
    "    lr_schedule : tf.keras.optimizers.schedules.LearningRateSchedule: learning rate schedule to use with the Adam optimizer.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model:  compiled Keras model\n",
    "    \"\"\"\n",
    "\n",
    "    model = build_model_dml(model_name, input_dim)\n",
    "    alpha = 1.0 / (1.0 + differential_weight * input_dim)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        loss={ 'y_pred': 'mse', 'dydx_pred' : grad_loss},\n",
    "        run_eagerly=None,\n",
    "        loss_weights=[alpha,1-alpha]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model,\n",
    "                prep_layer, \n",
    "                x_train, \n",
    "                y_train, \n",
    "                dydx_train=None,\n",
    "                epochs = config['epochs'],\n",
    "                batch_size = 1024,\n",
    "                x_true = None,\n",
    "                y_true = None,\n",
    "                dydx_true = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains a given model with the provided training data and preprocessing layer.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model: model to be trained.\n",
    "    prep_layer : NormalisationLayer: normalization layer used to scale the input and output data.\n",
    "    x_train : array or DataFrame: unscaled training input features.\n",
    "    y_train : array or DataFrame: unscaled training target values.\n",
    "    dydx_train : array or DataFrame, optional: unscaled training gradients. \n",
    "    epochs : int, optional: number of epochs to train the model.\n",
    "    batch_size : int, optional: batch size \n",
    "    x_true : array or DataFrame, optional: unscaled validation input features.\n",
    "    y_true : array or DataFrame, optional:  unscaled validation target values. \n",
    "    dydx_true : array or DataFrame, optional: unscaled validation gradients.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    history : keras.callbacks.History: record of training loss values and metrics for training and validation data.  \n",
    "    \"\"\"\n",
    "\n",
    "    history = model.fit(\n",
    "        prep_layer(x_train), [prep_layer.yScaled(y_train), prep_layer.dydxScaled(dydx_train)], \n",
    "        batch_size = batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=100)],\n",
    "        validation_data = (prep_layer(x_true), [prep_layer.yScaled(y_true), prep_layer.dydxScaled(dydx_true)]),\n",
    "        verbose=1\n",
    "        )\n",
    "    return history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
