{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library requirements \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_loss(hist):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss over epochs from a training history object.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    hist : A History object returned by the `fit` method of a Keras model.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.plot(hist.history['loss'], label='train_loss')\n",
    "    plt.plot(hist.history['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_vs_test(y_test, y_pred, title, x_label, y_label):\n",
    "    \"\"\"\n",
    "    Plots the predicted values against the true values, with a reference line y = x.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_test : array: true values (ground truth) to be compared against.\n",
    "    y_pred : array: predicted values from the model.\n",
    "    title : str: title of the plot.\n",
    "    x_label : str: x-axis label.\n",
    "    y_label : str: y-axis label.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # True values (x-axis) vs Predicted values (y-axis)\n",
    "    plt.scatter(y_test, y_pred, c='cyan', alpha=0.2, s=2, label='Predicted vs True')\n",
    "    # Plot the line y = x for reference\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=1, label='y = x')\n",
    "\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_pe_rel(df, trade_type, plot_type, title, x_label, y_label, ax):\n",
    "    \"\"\"\n",
    "    Plots the mean absolute or relative error for different mtm bins based on trade type.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame: input DataFrame containing the data to plot. It must have columns 'y_test', 'diff_abs', and 'diff_rel'.\n",
    "    trade_type : str: type of trade which determines the bin edges. ()'idx', 'idx_cs01', 'opt_delta', 'idx_theta', 'opt_theta', 'opt_deltastrike', 'opt_vega', 'opt')\n",
    "    plot_type : str: type of plot to create. ()'abs' for mean absolute error, 'rel' for mean relative error)\n",
    "    title : str: title of the plot.\n",
    "    x_label : str: label for the x-axis.\n",
    "    y_label : str: label for the y-axis.\n",
    "    ax : matplotlib.axes.Axes:  matplotlib Axes object where the plot will be drawn.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame: containing the mtm bins and their corresponding mean errors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define mtm bins \n",
    "    if trade_type == 'idx':\n",
    "        bin_edges = [-0.4, -0.35, -0.3, -0.25, -0.2, -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.15, 0.2]\n",
    "    elif trade_type == 'idx_cs01':\n",
    "        bin_edges = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0]\n",
    "    elif trade_type == 'opt_delta':\n",
    "        bin_edges = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5]\n",
    "    elif trade_type == 'idx_theta':\n",
    "        bin_edges = [ -0.006, -0.003666667, -0.00133,  0.001,  0.0033,  0.00566,  0.0080, 0.0103,  0.0126,  0.015]\n",
    "    elif trade_type == 'opt_theta':\n",
    "        bin_edges = [0.00025000, 0.01533333, 0.03091667, 0.04650000, 0.06208333, 0.07766667, 0.09325000, 0.10883333, 0.12441667, 0.14000000 ]\n",
    "    elif trade_type == 'opt_deltastrike':\n",
    "        bin_edges = [-4.81, -4.27261111, -3.73522222, -3.19783333, -2.66044444, -2.12305556, -1.58566667, -1.04827778, -0.51088889, -0.0055]\n",
    "    elif trade_type == 'opt_vega':\n",
    "        bin_edges = [0., 0.0009, 0.0018, 0.0027, 0.0036, 0.0045, 0.0054, 0.0063,  0.0072, 0.0081 ]\n",
    "    elif trade_type == 'opt':\n",
    "        bin_edges = [0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]\n",
    "    df = df.copy()  \n",
    "    df.loc[:, 'mtm_bin'] = pd.cut(df['y_test'], bins=bin_edges)\n",
    "\n",
    "    # Group by mtm bins and calculate the mean absolute error for each bin\n",
    "    if plot_type == 'abs':\n",
    "        bin_errors = df.groupby('mtm_bin')['diff_abs'].mean().reset_index()\n",
    "        ax.bar(bin_errors['mtm_bin'].astype(str), bin_errors['diff_abs'], color='lightblue')\n",
    "    elif plot_type == 'rel':\n",
    "        bin_errors = df.groupby('mtm_bin')['diff_rel'].mean().reset_index()\n",
    "        ax.bar(bin_errors['mtm_bin'].astype(str), bin_errors['diff_rel'], color='lightblue')\n",
    "\n",
    "    # Plot the histogram\n",
    "    ax.set_xlabel(x_label, fontsize=12)\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize=10)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "    return bin_errors\n",
    "\n",
    "def plot_pred_error_rel_twice(df1, df2, type, title1, title2, x=None):\n",
    "    \"\"\"\n",
    "    Plots the mean relative and absolute errors for two side-by-side subplots.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df1 : DataFrame: first input data DataFrame \n",
    "    df2 : DataFrame: second input data DataFrame\n",
    "    type : str: type of trade which determines the bin edges for mtm  ()'idx', 'idx_cs01', 'opt_delta', 'idx_theta', 'opt_theta', 'opt_deltastrike', 'opt_vega', 'opt')\n",
    "    title1 : str: title of the first subplot.\n",
    "    title2 : str: title of the second subplot.\n",
    "    x : str, optional: label for the x-axis. defaults to 'MtM Bin'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple containing two DataFrames:\n",
    "        - bin_errors1 : DataFrame with mean relative errors from the first DataFrame.\n",
    "        - bin_errors2 : DataFrame with mean raw errors from the second DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the subplots\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "    if x is None:\n",
    "        x_lab = 'Price Bin'\n",
    "    else:\n",
    "        x_lab = x\n",
    "\n",
    "    # Call the function twice for relative and abs diffs \n",
    "    bin_errors1 = plot_pe_rel(df1, type, 'rel', title=title1, x_label=x_lab, y_label='Mean Error (%)', ax=axes[0])\n",
    "    bin_errors2 = plot_pe_rel(df2, type, 'abs', title=title2, x_label=x_lab, y_label='Mean Raw Error', ax=axes[1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return(bin_errors1, bin_errors2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pe(ax, df_, plot_type, trade_type, title, x_label, y_label, size=0.05):\n",
    "     \n",
    "    \"\"\"\n",
    "    Plots prediction errors against true values for a given trade type and plot type.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes: Axes object where the plot will be drawn.\n",
    "    df_ : DataFrame: containing the data to plot. It must have columns 'y_test', 'diff_abs', and 'diff_rel'.\n",
    "    plot_type : str: type of plot to create. ('abs',  'rel') \n",
    "    trade_type : str: type of trade which determines the x-axis limits. ('idx', 'opt')\n",
    "    title : str: title of the plot.\n",
    "    x_label : str: label for the x-axis.\n",
    "    y_label : str: label for the y-axis.\n",
    "    size : float, optional: size of the scatter plot points. Default is 0.05.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if plot_type == 'abs':\n",
    "        y_data = df_['diff_abs']\n",
    "    elif plot_type == 'rel':\n",
    "        y_data = df_['diff_rel']\n",
    "\n",
    "    # Plot the data\n",
    "    ax.scatter(df_['y_test'], y_data, s=size, c='lightblue', alpha=0.2)\n",
    "    ax.axhline(0, color='r', linestyle='--')\n",
    "    ax.set_xlabel(x_label,fontsize=12)\n",
    "    ax.set_ylabel(y_label,fontsize=12)\n",
    "    ax.set_title(title, fontsize=13)\n",
    "    if trade_type == 'idx':\n",
    "        ax.set_xlim(-0.2, 0.2)\n",
    "    elif trade_type == 'opt': \n",
    "        ax.set_xlim(0.0, 0.2)   \n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='x', labelsize=10)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "def plot_pred_error_twice(df1, df2, trade_type, title1, title2, size=0.05, x=None):\n",
    "    \"\"\"\n",
    "    Plots relative and absolute prediction errors for two dataframes using subplots.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df1 : DataFrame: first input containing the data to plot.\n",
    "    df2 : DataFrame: second input containing the data to plot.\n",
    "    trade_type : str: type of trade which determines the x-axis limits. ('idx', 'opt')\n",
    "    title1 : str: title of the first subplot.\n",
    "    title2 : str: title of the second subplot.\n",
    "    size : float, optional: size of the scatter plot points. Default is 0.05.\n",
    "    x : str, optional: label for the x-axis. If not provided, defaults to 'Price'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        This function does not return any value. It displays two scatter plots of the prediction errors.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    if x is None:\n",
    "        x_lab = 'Price'\n",
    "    else:\n",
    "        x_lab = x\n",
    "\n",
    "    plot_pe(axes[0], df1, 'rel', trade_type, title1, x_lab, 'Relative Error (%)', size)\n",
    "    plot_pe(axes[1], df2, 'abs', trade_type, title2, x_lab, 'Raw Error', size)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_index_dml_fit(y_true, y_pred, dydx_true, dydx_pred, name1, name2, name3):\n",
    "    \"\"\"\n",
    "    Plots the predicted values against the true values and the predicted gradients against the true gradients\n",
    "    for two dimensions, using subplots.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array: true values (ground truth)\n",
    "    y_pred : array: predicted NN values \n",
    "    dydx_true : DataFrame: true gradients (ground truth)\n",
    "    dydx_pred : DataFrame: predicted gradients from the model.\n",
    "    name1 : str: title for the first subplot (predicted vs true values for MTM).\n",
    "    name2 : str: title for the second subplot (predicted vs true gradients for CS01).\n",
    "    name3 : str: title for the third subplot (predicted vs true gradients for THETA).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(21, 6))\n",
    "\n",
    "    # Plot 1: y_pred vs y_true\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(y_true, y_pred, s=1, c='cyan', label='Predicted vs True')\n",
    "    plt.xlabel('True Values', fontsize=14)\n",
    "    plt.ylabel('Predicted Values', fontsize=14)\n",
    "    plt.title(f'{name1}: pred vs true', fontsize=16)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--', label='y = x')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Plot 2: dydx_pred vs dydx_true (dimension 0)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    dimension = 0\n",
    "    plt.scatter(dydx_true.iloc[:, dimension], dydx_pred[:, dimension], s=1, c='cyan', label='Predicted vs True')\n",
    "    plt.xlabel('True Gradients', fontsize=14)\n",
    "    plt.ylabel('Predicted Gradients', fontsize=14)\n",
    "    plt.title(f'{name2}: pred vs true ', fontsize=16)\n",
    "    plt.plot([min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], \n",
    "             [min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], 'r--', label='y = x')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Plot 3: dydx_pred vs dydx_true (dimension 1)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    dimension = 1\n",
    "    plt.scatter(dydx_true.iloc[:, dimension], dydx_pred[:, dimension], s=1, c='cyan', label='Predicted vs True')\n",
    "    plt.xlabel('True Gradients', fontsize=14)\n",
    "    plt.ylabel('Predicted Gradients', fontsize=14)\n",
    "    plt.title(f'{name3}: pred vs true', fontsize=16)\n",
    "    plt.plot([min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], \n",
    "             [min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], 'r--', label='y = x')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def plot_error_bars(result, type, within_val_gt, within_val_nn):\n",
    "    \"\"\"\n",
    "    Plots the absolute errors within and outside error bars for ground truth and predicted MtM values.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result : DataFrame: input DataFrame containing the results data. It must have columns ()'mtm_t', 'mtm_t_pred', 'mtm_t_within_nn_error', and 'mtm_t_pred_within_nn_error')\n",
    "    \n",
    "    type : str: trade type ('Index', 'Option')\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    " \n",
    "    \"\"\"\n",
    "    # Create two plots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Within Error Bars ' + within_val_gt, markerfacecolor='lightgreen', markersize=10, alpha=0.5),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Outside Error Bars', markerfacecolor='purple', markersize=10, alpha=0.5)\n",
    "    ]\n",
    "\n",
    "    # First plot: using mtm_t_within_nn_error\n",
    "    within_error = result[result['mtm_t_within_nn_error']]\n",
    "    outside_error = result[~result['mtm_t_within_nn_error']]\n",
    "\n",
    "    #ax1.scatter(within_error['mtm_t'], np.abs(within_error['mtm_t'] - within_error['mtm_t_pred']), s=0.5, c='lightgreen', alpha=0.5)\n",
    "    #ax1.scatter(outside_error['mtm_t'], np.abs(outside_error['mtm_t'] - outside_error['mtm_t_pred']), s=0.5, c='purple', alpha=0.5)\n",
    "    ax1.scatter(within_error['mtm_t'], within_error['mtm_t'] - within_error['mtm_t_pred'], s=0.5, c='lightgreen', alpha=0.5)\n",
    "    ax1.scatter(outside_error['mtm_t'], outside_error['mtm_t'] - outside_error['mtm_t_pred'], s=0.5, c='purple', alpha=0.5)\n",
    "    ax1.set_xlabel('Price', fontsize=11)\n",
    "    ax1.set_ylabel('Raw Errors', fontsize=11)\n",
    "    ax1.legend(handles=legend_elements)\n",
    "    ax1.set_title(f'{type} Ground Truth Price within Error Bars',  fontsize=13)\n",
    "\n",
    "    # Second plot: using mtm_t_pred_within_nn_error\n",
    "    within_error = result[result['mtm_t_pred_within_nn_error']]\n",
    "    outside_error = result[~result['mtm_t_pred_within_nn_error']]\n",
    "\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', label='Within Error Bars ' + within_val_nn, markerfacecolor='lightgreen', markersize=10, alpha=0.5),\n",
    "        Line2D([0], [0], marker='o', color='w', label='Outside Error Bars', markerfacecolor='purple', markersize=10, alpha=0.5)\n",
    "    ]\n",
    "\n",
    "    #ax2.scatter(within_error['mtm_t'], np.abs(within_error['mtm_t'] - within_error['mtm_t_pred']), s=0.5, c='lightgreen', alpha=0.5)\n",
    "    #ax2.scatter(outside_error['mtm_t'], np.abs(outside_error['mtm_t'] - outside_error['mtm_t_pred']), s=0.5, c='purple', alpha=0.5)\n",
    "    ax2.scatter(within_error['mtm_t'], within_error['mtm_t'] - within_error['mtm_t_pred'], s=0.5, c='lightgreen', alpha=0.5)\n",
    "    ax2.scatter(outside_error['mtm_t'], outside_error['mtm_t'] - outside_error['mtm_t_pred'], s=0.5, c='purple', alpha=0.5)\n",
    "    ax2.set_xlabel('Price', fontsize=11)\n",
    "    ax2.set_ylabel('Raw Errors', fontsize=11)\n",
    "    ax2.legend(handles=legend_elements)\n",
    "    ax2.set_title(f'{type} NN Predicted Price within Error Bars', fontsize=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_true_misses_and_rel_size(result, error_column='mtm_t_within_nn_error', miss_column='true_misses', rel_size_column='rel_size', type='Index'):\n",
    "    \"\"\"\n",
    "    Plots the percentage of points falling outside the error bars for specified miss categories and rel_size distribution.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result : DataFrame: input DataFrame containing the results data. It must have columns specified by `error_column`, `miss_column`, and `rel_size_column`.\n",
    "    error_column : str, optional: name of the column indicating whether a point is within the error bars. Default is 'mtm_t_within_nn_error'.\n",
    "    miss_column : str, optional: name of the column indicating the miss distances if not inside the error bar. Default is 'true_misses'.\n",
    "    rel_size_column : str, optional: name of the column for rel_size data. Default is 'rel_size'.\n",
    "    type : str, optional: trade type ('Index', 'Option')\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    def calculate_percentages(result, error_column, miss_column):\n",
    "        # Filter rows based on the specified error column\n",
    "        filtered_results = result[result[error_column] == False]\n",
    "\n",
    "        # Define boundary bands\n",
    "        bands = [1, 3, 5, 10, 20, 50]\n",
    "        band_labels = ['0-1%', '1-3%', '3-5%', '5-10%', '10-20%', '20-50%', '>50%']\n",
    "\n",
    "        # Count the number of points within each band\n",
    "        counts = [\n",
    "            np.sum((filtered_results[miss_column] > bands[i-1]) & (filtered_results[miss_column] <= bands[i]))\n",
    "            if i > 0 else np.sum(filtered_results[miss_column] <= bands[i])\n",
    "            for i in range(len(bands))\n",
    "        ]\n",
    "        # Count points greater than the last boundary\n",
    "        counts.append(np.sum(filtered_results[miss_column] > bands[-1]))\n",
    "\n",
    "        # Calculate percentages\n",
    "        total_points = len(filtered_results)\n",
    "        percentages = [(count / total_points) * 100 for count in counts]\n",
    "\n",
    "        return percentages, band_labels\n",
    "\n",
    "    # Calculate percentages for true_misses\n",
    "    percentages, band_labels = calculate_percentages(result, error_column, miss_column)\n",
    "\n",
    "    # Define bins and labels for rel_size\n",
    "    bins = [0, 1, 3, 5, 10, 20, 50, float('inf')]\n",
    "    rel_size_labels = ['0-1%', '1-3%', '3-5%', '5-10%', '10-20%', '20-50%', '>50%']\n",
    "\n",
    "    # Bin the rel_size column and calculate the percentage\n",
    "    result['rel_size_bin'] = pd.cut(result[rel_size_column], bins=bins, labels=rel_size_labels, right=False)\n",
    "    rel_size_percentages = result['rel_size_bin'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot true_misses percentages\n",
    "    bars1 = axs[0].bar(range(len(percentages)), percentages, color='purple', edgecolor='black', alpha=0.7)\n",
    "    for bar, percentage in zip(bars1, percentages):\n",
    "        height = bar.get_height()\n",
    "        axs[0].annotate(f'{percentage:.2f}%',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    axs[0].set_xticks(range(len(band_labels)))\n",
    "    axs[0].set_xticklabels(band_labels)\n",
    "    axs[0].set_xlabel('Outside Distance / Width Error Bar', fontsize=11)\n",
    "    axs[0].set_ylabel('Percentage of Points', fontsize=11)\n",
    "    axs[0].set_title(f'{type} Ground Truth Price outside Error Bars', fontsize=12)\n",
    "\n",
    "    # Plot rel_size percentages\n",
    "    bars2 = axs[1].bar(range(len(rel_size_percentages)), rel_size_percentages, color='purple', edgecolor='black', alpha=0.7)\n",
    "    for bar, percentage in zip(bars2, rel_size_percentages):\n",
    "        height = bar.get_height()\n",
    "        axs[1].annotate(f'{percentage:.2f}%',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    axs[1].set_xticks(range(len(rel_size_labels)))\n",
    "    axs[1].set_xticklabels(rel_size_labels)\n",
    "    axs[1].set_xlabel('Error Bar Size / Price', fontsize=11)\n",
    "    axs[1].set_ylabel('Percentage of Points', fontsize=11)\n",
    "    axs[1].set_title(f'{type} Distribution of Error Bar Sizes as Percenatge of Price ', fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_option_dml_fit(y_true, y_pred, dydx_true, dydx_pred, name1, name2, name3, name4, name5):\n",
    "    \"\"\"\n",
    "    Plots the predicted values against the true values and the predicted gradients against the true gradients\n",
    "    for DELTA, THETA, DELTASTRIKE, VEGA \n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array: true values (ground truth)\n",
    "    y_pred : array: predicted values from the model.\n",
    "    dydx_true : DataFrame: true gradients (ground truth)\n",
    "    dydx_pred : DataFrame: predicted gradients from the model.\n",
    "    name1 : str: title for the first subplot (predicted vs true values MTM).\n",
    "    name2 : str: title for the second subplot (predicted vs true gradients for DELTA).\n",
    "    name3 : str: title for the third subplot (predicted vs true gradients for THETA).\n",
    "    name4 : str: title for the fourth subplot (predicted vs true gradients for DELTASTRIKE).\n",
    "    name5 : str: title for the fifth subplot (predicted vs true gradients for VEGA).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(21, 12))\n",
    "\n",
    "    # Plot 1: y_pred vs y_true\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.scatter(y_true, y_pred, s=1, c='cyan', label='Predicted vs True')\n",
    "    plt.xlabel('True Values',fontsize=14)\n",
    "    plt.ylabel('Predicted Values',fontsize=14)\n",
    "    plt.title(f'{name1}: pred vs true',fontsize=16)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], 'r--', label='y = x')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Plot 2: dydx_pred vs dydx_true (dimension 0)\n",
    "    plt.subplot(2, 3, 2)\n",
    "    dimension = 0\n",
    "    plt.scatter(dydx_true.iloc[:, dimension], dydx_pred[:, dimension], s=1, c='cyan', label='Predicted vs True')\n",
    "    plt.xlabel('True Gradients',fontsize=14)\n",
    "    plt.ylabel('Predicted Gradients',fontsize=14)\n",
    "    plt.title(f'{name2}: pred vs true',fontsize=16)\n",
    "    plt.plot([min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], \n",
    "             [min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], 'r--', label='y = x')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Plot 3: dydx_pred vs dydx_true (dimension 1)\n",
    "    plt.subplot(2, 3, 3)\n",
    "    dimension = 1\n",
    "    plt.scatter(dydx_true.iloc[:, dimension], dydx_pred[:, dimension], s=1, c='cyan', label='Predicted vs True')\n",
    "    plt.xlabel('True Gradients',fontsize=14)\n",
    "    plt.ylabel('Predicted Gradients',fontsize=14)\n",
    "    plt.title(f'{name3}: pred vs true',fontsize=16)\n",
    "    plt.plot([min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], \n",
    "             [min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], 'r--', label='y = x')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Plot 4: dydx_pred vs dydx_true (dimension 2)\n",
    "    plt.subplot(2, 3, 4)\n",
    "    dimension = 2\n",
    "    plt.scatter(dydx_true.iloc[:, dimension], dydx_pred[:, dimension], s=1, c='cyan', label='Predicted vs True')\n",
    "    plt.xlabel('True Gradients',fontsize=14)\n",
    "    plt.ylabel('Predicted Gradients',fontsize=14)\n",
    "    plt.title(f'{name4}: pred vs true',fontsize=16)\n",
    "    plt.plot([min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], \n",
    "             [min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], 'r--', label='y = x')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # Plot 5: dydx_pred vs dydx_true (dimension 3)\n",
    "    plt.subplot(2, 3, 5)\n",
    "    dimension = 3\n",
    "    plt.scatter(dydx_true.iloc[:, dimension], dydx_pred[:, dimension], s=1, c='cyan', label='Predicted vs True')\n",
    "    plt.xlabel('True Gradients',fontsize=14)\n",
    "    plt.ylabel('Predicted Gradients',fontsize=14)\n",
    "    plt.title(f'{name5}: pred vs true',fontsize=16)\n",
    "    plt.plot([min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], \n",
    "             [min(dydx_true.iloc[:, dimension]), max(dydx_true.iloc[:, dimension])], 'r--', label='y = x')\n",
    "    plt.legend()\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
